{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Understanding the Dataset\n",
    "# The dataset contains 16 columns related to video game sales, ratings, and metadata. Key observations:\n",
    "\n",
    "# Target Variable: Global_Sales (Total sales worldwide)\n",
    "# Feature Candidates: Platform, Year_of_Release, Genre, Publisher, Critic_Score, etc.\n",
    "# Missing Data: Some columns like Critic_Score, User_Score, User_Count, and Developer have missing values.\n",
    "# Data Types: User_Score is stored as an object, likely requiring conversion.\n",
    "# Next, I'll clean the dataset and prepare it for training. ​​\n",
    "\n",
    "# Step 2: Preparing Data for Training\n",
    "# Target Variable: Global_Sales\n",
    "# Features: All other numerical and encoded categorical columns\n",
    "# Now, I'll split the dataset into training and testing sets and train a Regression Tree Model (Decision Tree Regressor). ​​\n",
    "\n",
    "# Step 3: Model Evaluation\n",
    "# The Decision Tree Regressor gives the following performance:\n",
    "\n",
    "# Mean Absolute Error (MAE): 0.0606\n",
    "# Mean Squared Error (MSE): 0.6223\n",
    "# R² Score: 0.8493 (Indicates a good fit, explaining ~85% of variance)\n",
    "# The model performs well, but we can improve it by hyperparameter tuning or using Random Forest Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be following the below steps to solve this problem:\n",
    "\n",
    "# Importing the libraries\n",
    "\n",
    "# Using some pre-defined utility functions\n",
    "\n",
    "# Loading the data\n",
    "\n",
    "# Cleaning the data\n",
    "\n",
    "# Dividing the dataset into training and test dataset\n",
    "\n",
    "# using train_test_split in the ratio 70:30\n",
    "# Training several models and analyzing their performance to select a model\n",
    "\n",
    "# Fine-tuning the model by finding the best hyper-parameters and features\n",
    "\n",
    "# Evaluating selected model using test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[13]:\n",
    "\n",
    "\n",
    "# Please import the required libraries as mentioned below:\n",
    "\n",
    "# Import numpy as np\n",
    "# Import pandas as pd\n",
    "# From sklearn import preprocessing\n",
    "# Please import StandardScaler from Scikit Learn - preprocessing\n",
    "# Please import mean_squared_error from Scikit Learn - metrics\n",
    "# Please import linear_model from Scikit Learn\n",
    "# Please import matplotlib's pyplot as plt\n",
    "# Import os\n",
    "# set random seed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "with open('./Video_Games_Sales.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read(100000))  # Read first 100000 bytes\n",
    "    print(result['encoding'])\n",
    "\n",
    "# data = pd.read_csv('./water_quality.csv', encoding=result['encoding'])\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# data = pd.read_csv('your_file.csv', encoding='ISO-8859-1')\n",
    "# # OR\n",
    "# data = pd.read_csv('your_file.csv', encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Global_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16450.000000</td>\n",
       "      <td>16719.000000</td>\n",
       "      <td>16719.000000</td>\n",
       "      <td>16719.000000</td>\n",
       "      <td>16719.000000</td>\n",
       "      <td>16719.000000</td>\n",
       "      <td>8137.000000</td>\n",
       "      <td>8137.000000</td>\n",
       "      <td>7590.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2006.487356</td>\n",
       "      <td>0.263330</td>\n",
       "      <td>0.145025</td>\n",
       "      <td>0.077602</td>\n",
       "      <td>0.047332</td>\n",
       "      <td>0.533543</td>\n",
       "      <td>68.967679</td>\n",
       "      <td>26.360821</td>\n",
       "      <td>162.229908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.878995</td>\n",
       "      <td>0.813514</td>\n",
       "      <td>0.503283</td>\n",
       "      <td>0.308818</td>\n",
       "      <td>0.186710</td>\n",
       "      <td>1.547935</td>\n",
       "      <td>13.938165</td>\n",
       "      <td>18.980495</td>\n",
       "      <td>561.282326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1980.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2003.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2007.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2010.000000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>41.360000</td>\n",
       "      <td>28.960000</td>\n",
       "      <td>10.220000</td>\n",
       "      <td>10.570000</td>\n",
       "      <td>82.530000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>10665.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year_of_Release      NA_Sales      EU_Sales      JP_Sales  \\\n",
       "count     16450.000000  16719.000000  16719.000000  16719.000000   \n",
       "mean       2006.487356      0.263330      0.145025      0.077602   \n",
       "std           5.878995      0.813514      0.503283      0.308818   \n",
       "min        1980.000000      0.000000      0.000000      0.000000   \n",
       "25%        2003.000000      0.000000      0.000000      0.000000   \n",
       "50%        2007.000000      0.080000      0.020000      0.000000   \n",
       "75%        2010.000000      0.240000      0.110000      0.040000   \n",
       "max        2020.000000     41.360000     28.960000     10.220000   \n",
       "\n",
       "        Other_Sales  Global_Sales  Critic_Score  Critic_Count    User_Count  \n",
       "count  16719.000000  16719.000000   8137.000000   8137.000000   7590.000000  \n",
       "mean       0.047332      0.533543     68.967679     26.360821    162.229908  \n",
       "std        0.186710      1.547935     13.938165     18.980495    561.282326  \n",
       "min        0.000000      0.010000     13.000000      3.000000      4.000000  \n",
       "25%        0.000000      0.060000     60.000000     12.000000     10.000000  \n",
       "50%        0.010000      0.170000     71.000000     21.000000     24.000000  \n",
       "75%        0.030000      0.470000     79.000000     36.000000     81.000000  \n",
       "max       10.570000     82.530000     98.000000    113.000000  10665.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"./Video_Games_Sales.csv\"\n",
    "video_sales_Data =  pd.read_csv(filepath, encoding='ISO-8859-1')\n",
    "video_sales_Data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16719 entries, 0 to 16718\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Name             16717 non-null  object \n",
      " 1   Platform         16719 non-null  object \n",
      " 2   Year_of_Release  16450 non-null  float64\n",
      " 3   Genre            16717 non-null  object \n",
      " 4   Publisher        16665 non-null  object \n",
      " 5   NA_Sales         16719 non-null  float64\n",
      " 6   EU_Sales         16719 non-null  float64\n",
      " 7   JP_Sales         16719 non-null  float64\n",
      " 8   Other_Sales      16719 non-null  float64\n",
      " 9   Global_Sales     16719 non-null  float64\n",
      " 10  Critic_Score     8137 non-null   float64\n",
      " 11  Critic_Count     8137 non-null   float64\n",
      " 12  User_Score       10015 non-null  object \n",
      " 13  User_Count       7590 non-null   float64\n",
      " 14  Developer        10096 non-null  object \n",
      " 15  Rating           9950 non-null   object \n",
      "dtypes: float64(9), object(7)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "video_sales_Data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with a missing target variable (Global Sales)\n",
    "df = video_sales_Data.dropna(subset=[\"Global_Sales\",\"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Electronic Arts\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Publisher\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/usr/local/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/mitgoldmedalist20201634/.local/lib/python3.6/site-packages/pandas/core/series.py:4536: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n",
      "/usr/local/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#Fill the missing numerical values with median or mode\n",
    "df[\"User_Score\"] = pd.to_numeric(df[\"User_Score\"],errors=\"coerce\")\n",
    "df[\"User_Score\"] = df[\"User_Score\"].fillna(df[\"User_Score\"].median())\n",
    "\n",
    "df[\"User_Count\"] = pd.to_numeric(df[\"User_Count\"],errors=\"coerce\")\n",
    "df[\"User_Count\"] = df[\"User_Count\"].fillna(df[\"User_Count\"].median())\n",
    "\n",
    "df['Year_of_Release'].fillna(df['Year_of_Release'].mode()[0], inplace=True)\n",
    "\n",
    "\n",
    "df[\"Critic_Score\"] = pd.to_numeric(df[\"Critic_Score\"],errors=\"coerce\")\n",
    "df[\"Critic_Score\"] = df[\"Critic_Score\"].fillna(df[\"Critic_Score\"].median())\n",
    "\n",
    "df[\"Critic_Count\"] = pd.to_numeric(df[\"Critic_Count\"],errors=\"coerce\")\n",
    "df[\"Critic_Count\"] = df[\"Critic_Count\"].fillna(df[\"Critic_Count\"].median()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Drop less useful columns \n",
    "df = df.drop(columns=[\"Developer\",\"NA_Sales\",\"EU_Sales\",\"JP_Sales\",\"Other_Sales\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Publisher      54\n",
       "Rating       6767\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()[df.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert categorical variables into numerical using One Hot Encoding\n",
    "# df = pd.get_dummies(df, columns=[\"Publisher\",\"Platform\", \"Genre\", \"Rating\"],drop_first=True)\n",
    "\n",
    "# df.isna().sum()[df.isna().sum() > 0]\n",
    "\n",
    "df['Publisher'].fillna(df['Publisher'].mode()[0], inplace=True)\n",
    "\n",
    "# df[\"Platform\"] = df[\"Platform\"].fillna(df[\"Platform\"].mode())\n",
    "\n",
    "df['Genre'].fillna(df['Genre'].mode()[0], inplace=True)\n",
    "\n",
    "df['Rating'].fillna(df['Rating'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Name  Year_of_Release  Global_Sales  \\\n",
      "0                         Wii Sports           2006.0         82.53   \n",
      "1                  Super Mario Bros.           1985.0         40.24   \n",
      "2                     Mario Kart Wii           2008.0         35.52   \n",
      "3                  Wii Sports Resort           2009.0         32.77   \n",
      "4           Pokemon Red/Pokemon Blue           1996.0         31.37   \n",
      "...                              ...              ...           ...   \n",
      "16714  Samurai Warriors: Sanada Maru           2016.0          0.01   \n",
      "16715               LMA Manager 2007           2006.0          0.01   \n",
      "16716        Haitaka no Psychedelica           2016.0          0.01   \n",
      "16717               Spirits & Spells           2003.0          0.01   \n",
      "16718            Winning Post 8 2016           2016.0          0.01   \n",
      "\n",
      "       Critic_Score  Critic_Count  User_Score  User_Count  \\\n",
      "0              76.0          51.0         8.0       322.0   \n",
      "1              71.0          21.0         7.5        24.0   \n",
      "2              82.0          73.0         8.3       709.0   \n",
      "3              80.0          73.0         8.0       192.0   \n",
      "4              71.0          21.0         7.5        24.0   \n",
      "...             ...           ...         ...         ...   \n",
      "16714          71.0          21.0         7.5        24.0   \n",
      "16715          71.0          21.0         7.5        24.0   \n",
      "16716          71.0          21.0         7.5        24.0   \n",
      "16717          71.0          21.0         7.5        24.0   \n",
      "16718          71.0          21.0         7.5        24.0   \n",
      "\n",
      "       Publisher_1C Company  Publisher_20th Century Fox Video Games  \\\n",
      "0                         0                                       0   \n",
      "1                         0                                       0   \n",
      "2                         0                                       0   \n",
      "3                         0                                       0   \n",
      "4                         0                                       0   \n",
      "...                     ...                                     ...   \n",
      "16714                     0                                       0   \n",
      "16715                     0                                       0   \n",
      "16716                     0                                       0   \n",
      "16717                     0                                       0   \n",
      "16718                     0                                       0   \n",
      "\n",
      "       Publisher_2D Boy  ...  Genre_Simulation  Genre_Sports  Genre_Strategy  \\\n",
      "0                     0  ...                 0             1               0   \n",
      "1                     0  ...                 0             0               0   \n",
      "2                     0  ...                 0             0               0   \n",
      "3                     0  ...                 0             1               0   \n",
      "4                     0  ...                 0             0               0   \n",
      "...                 ...  ...               ...           ...             ...   \n",
      "16714                 0  ...                 0             0               0   \n",
      "16715                 0  ...                 0             1               0   \n",
      "16716                 0  ...                 0             0               0   \n",
      "16717                 0  ...                 0             0               0   \n",
      "16718                 0  ...                 1             0               0   \n",
      "\n",
      "       Rating_E  Rating_E10+  Rating_EC  Rating_K-A  Rating_M  Rating_RP  \\\n",
      "0             1            0          0           0         0          0   \n",
      "1             1            0          0           0         0          0   \n",
      "2             1            0          0           0         0          0   \n",
      "3             1            0          0           0         0          0   \n",
      "4             1            0          0           0         0          0   \n",
      "...         ...          ...        ...         ...       ...        ...   \n",
      "16714         1            0          0           0         0          0   \n",
      "16715         1            0          0           0         0          0   \n",
      "16716         1            0          0           0         0          0   \n",
      "16717         1            0          0           0         0          0   \n",
      "16718         1            0          0           0         0          0   \n",
      "\n",
      "       Rating_T  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "...         ...  \n",
      "16714         0  \n",
      "16715         0  \n",
      "16716         0  \n",
      "16717         0  \n",
      "16718         0  \n",
      "\n",
      "[16717 rows x 635 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Initialize OneHotEncoder\n",
    "# encoder = OneHotEncoder(sparse=False, drop='first')  # drop='first' avoids dummy variable trap\n",
    "\n",
    "# # Fit and transform data\n",
    "# encoded_array = encoder.fit_transform(df[[\"Publisher\",\"Platform\", \"Genre\", \"Rating\"]])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names([\"Publisher\",\"Platform\", \"Genre\", \"Rating\"]))\n",
    "\n",
    "# # Concatenate with original DataFrame\n",
    "# df_final = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "# print(df_final)\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df, columns=[\"Publisher\",\"Platform\", \"Genre\", \"Rating\"], drop_first=True)\n",
    "\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Display the clean dataset info \n",
    "# df_final.info(), df_final.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# # Re-load dataset\n",
    "# df = pd.read_csv(\"./Video_Games_Sales.csv\")\n",
    "\n",
    "# # Drop columns with more than 50% missing values\n",
    "# missing_threshold = 0.5 * len(df)\n",
    "# df = df.dropna(thresh=missing_threshold, axis=1)\n",
    "\n",
    "# # Fill missing values for numerical columns with median\n",
    "# num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "# df[num_cols] = df[num_cols].apply(lambda col: col.fillna(col.median()))\n",
    "\n",
    "# # Fill missing values for categorical columns with most frequent value (mode)\n",
    "# cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "# df[cat_cols] = df[cat_cols].apply(lambda col: col.fillna(col.mode()[0]))\n",
    "\n",
    "# # Verify if any NaN values remain\n",
    "# nan_counts_after = df.isna().sum().sum()\n",
    "# nan_counts_after\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# End to End Project - Video Games Sales - Basic - Divide into training/ test dataset\n",
    "# Now, since we have cleaned the video_sales_Data data set, let us split it into Training and Test data sets into 70:30 ratio using scikit-learn's train_test_split() function.\n",
    "\n",
    "# Also, train_test_split() function uses 'Random Sampling', hence resulting train_set and test_set data sets have to be sorted by dayCount. Random Sampling may not be the best way to split the data, what other types of best Sampling method you can think of?\n",
    "\n",
    "# We will also define an utility function named display_scores. This function is used to calculate the basics stats of observed scores from cross-validation of models. Please copy this function in your code, we will be using it often in this project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5127661483253588, 4.505380352870813, -0.09094197796573522)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision Tree Regressor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Define features and target\n",
    "X = df.drop(columns=[\"Global_Sales\"])\n",
    "y = df[\"Global_Sales\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Apply Label Encoding to categorical columns\n",
    "label_encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le  # Store encoders for future use\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "(mae, mse, r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5662084287360779, 3.863633167691587, 0.06445199295777637)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Linear Regression Model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Re-load dataset\n",
    "# df = pd.read_csv(\"./Video_Games_Sales.csv\")\n",
    "\n",
    "# # Drop columns with more than 50% missing values\n",
    "# missing_threshold = 0.5 * len(df)\n",
    "# df = df.dropna(thresh=missing_threshold, axis=1)\n",
    "\n",
    "# # Fill missing values for numerical columns with median\n",
    "# num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "# df[num_cols] = df[num_cols].apply(lambda col: col.fillna(col.median()))\n",
    "\n",
    "# # Fill missing values for categorical columns with most frequent value (mode)\n",
    "# cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "# df[cat_cols] = df[cat_cols].apply(lambda col: col.fillna(col.mode()[0]))\n",
    "\n",
    "# Define features and target\n",
    "X1 = df_encoded.drop(columns=[\"Global_Sales\",\"Name\"])\n",
    "y1 = df_encoded[\"Global_Sales\"]\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr_model.predict(X_test)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "mae_lr = mean_absolute_error(y_test, y_pred)\n",
    "mse_lr = mean_squared_error(y_test, y_pred)\n",
    "r2_lr = r2_score(y_test, y_pred)\n",
    "\n",
    "(mae_lr, mse_lr, r2_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4273270260167463, 2.9695939295635836, 0.28093647560547474)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Regression \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = df.drop(columns=[\"Global_Sales\"])\n",
    "y = df[\"Global_Sales\"]\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "(mae_rf, mse_rf, r2_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-1.164   \u001b[0m | \u001b[0m21.85    \u001b[0m | \u001b[0m4.803    \u001b[0m | \u001b[0m7.856    \u001b[0m | \u001b[0m319.4    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-1.243   \u001b[0m | \u001b[0m12.02    \u001b[0m | \u001b[0m1.624    \u001b[0m | \u001b[0m2.465    \u001b[0m | \u001b[0m439.8    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m-1.144   \u001b[0m | \u001b[95m32.05    \u001b[0m | \u001b[95m3.832    \u001b[0m | \u001b[95m2.165    \u001b[0m | \u001b[95m486.5    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-1.23    \u001b[0m | \u001b[0m42.46    \u001b[0m | \u001b[0m1.849    \u001b[0m | \u001b[0m3.455    \u001b[0m | \u001b[0m132.5    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-1.153   \u001b[0m | \u001b[0m18.69    \u001b[0m | \u001b[0m3.099    \u001b[0m | \u001b[0m5.456    \u001b[0m | \u001b[0m181.1    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-1.163   \u001b[0m | \u001b[0m18.16    \u001b[0m | \u001b[0m2.583    \u001b[0m | \u001b[0m6.417    \u001b[0m | \u001b[0m181.7    \u001b[0m |\n",
      "=========================================================================\n",
      "Best Parameters: {'target': -1.1440179517081694, 'params': {'max_depth': 32.0501755284444, 'min_samples_leaf': 3.832290311184182, 'min_samples_split': 2.1646759543664196, 'n_estimators': 486.45943347289744}}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the function to optimize\n",
    "def rf_cv(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=int(n_estimators),\n",
    "        max_depth=int(max_depth),\n",
    "        min_samples_split=int(min_samples_split),\n",
    "        min_samples_leaf=int(min_samples_leaf),\n",
    "        random_state=42\n",
    "    )\n",
    "    return np.mean(cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error'))\n",
    "\n",
    "# Define the parameter search space\n",
    "param_bounds = {\n",
    "    'n_estimators': (50, 500),\n",
    "    'max_depth': (5, 50),\n",
    "    'min_samples_split': (2, 10),\n",
    "    'min_samples_leaf': (1, 5)\n",
    "}\n",
    "\n",
    "# Initialize Bayesian Optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=rf_cv,  # Function to optimize\n",
    "    pbounds=param_bounds,  # Hyperparameter ranges\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "optimizer.maximize(init_points=5, n_iter=1)  # 5 random initial points, 20 optimization steps\n",
    "\n",
    "# Best Parameters\n",
    "print(\"Best Parameters:\", optimizer.max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# # Create a dictionary of hyperparameters for GridSearchCV:\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],  # Number of trees\n",
    "#     'max_depth': [None, 10, 20, 30],  # Depth of trees\n",
    "#     'min_samples_split': [2, 5, 10],  # Minimum samples per split\n",
    "#     'min_samples_leaf': [1, 2, 4]  # Minimum samples per leaf\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now, apply Grid Search on your RandomForestRegressor:\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=RandomForestRegressor(random_state=42),\n",
    "#     param_grid=param_grid,\n",
    "#     cv=5,  # 5-fold cross-validation\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     n_jobs=-1,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Best Parameters & Score\n",
    "\n",
    "# print(\"Best Parameters:\", optimizer.max)\n",
    "# print(\"Best RMSE:\", (-optimizer.max.best_score_) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_int = {k: int(v) for k, v in optimizer.max[\"params\"].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 1.7357584534647064\n"
     ]
    }
   ],
   "source": [
    "# Once the best hyperparameters are found, retrain your model with them:\n",
    "\n",
    "\n",
    "best_model = RandomForestRegressor(**best_params_int, random_state=42)\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate again\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "print(\"Final RMSE:\", mse_best ** 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --force-reinstall --no-cache-dir shap numba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 13371/13373 [58:54<00:00]        "
     ]
    },
    {
     "ename": "ExplainerError",
     "evalue": "Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 14.908616, while the model output was 14.756196. If this difference is acceptable you can set check_additivity=False to disable this check.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExplainerError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0e241e8e975c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create SHAP Explainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Summary Plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, y, interactions, check_additivity)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minteractions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_call\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_additivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_additivity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapproximate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproximate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# put outputs at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, y, tree_limit, approximate, check_additivity, from_call)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_shap_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_additivity\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_additivity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36massert_additivity\u001b[0;34m(self, phi, model_output)\u001b[0m\n\u001b[1;32m    542\u001b[0m                 \u001b[0mcheck_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m             \u001b[0mcheck_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36mcheck_sum\u001b[0;34m(sum_val, model_output)\u001b[0m\n\u001b[1;32m    536\u001b[0m                            \u001b[0;34m\" was %f, while the model output was %f. If this difference is acceptable\"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                            \u001b[0;34m\" you can set check_additivity=False to disable this check.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mExplainerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExplainerError\u001b[0m: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 14.908616, while the model output was 14.756196. If this difference is acceptable you can set check_additivity=False to disable this check."
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create SHAP Explainer\n",
    "explainer = shap.Explainer(best_model, X_train)\n",
    "shap_values = explainer(X_train)\n",
    "\n",
    "# Summary Plot\n",
    "shap.summary_plot(shap_values, X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(y_test, y_pred, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5, color=\"blue\", label=\"Predicted vs Actual\")\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color=\"red\", linestyle=\"--\", label=\"Perfect Fit\")\n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for Decision Tree\n",
    "plot_predictions(y_test, y_pred, \"Decision Tree: Predicted vs Actual\")\n",
    "\n",
    "# Plot for Linear Regression\n",
    "plot_predictions(y_test, y_pred_lr, \"Linear Regression: Predicted vs Actual\")\n",
    "\n",
    "# Plot for Random Forest\n",
    "plot_predictions(y_test, y_pred_rf, \"Random Forest: Predicted vs Actual\")\n",
    "\n",
    "# Plot for Best Random Forest Model (Grid Search)\n",
    "plot_predictions(y_test, y_pred_best, \"Tuned Random Forest: Predicted vs Actual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
